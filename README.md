# ğŸ§  Temporal LLaVA Habitat

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/)
[![Contributions](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](../../issues)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

_Augmenting Vision-Language Models with Temporal Reasoning for Embodied AI_

ğŸ”— [Demo](#-demo) Â· [Installation](#-installation) Â· [Usage](#-usage) Â· [Benchmarks](#-benchmarks) Â· [Research](#-research) Â· [Citation](#-citation) Â· [Future Work](#-future-work)

---

## âœ¨ Overview  

Recent advances in **vision-language models (VLMs)** like LLaVA (Large Language and Vision Assistant) have revolutionized **static image understanding**. However, embodied AI agents â€” whether a household robot or an aerial drone must act across **time-dependent sequences of events**.  

This repository extends [Habitat-Sim](https://github.com/facebookresearch/habitat-sim) with **temporal reasoning capabilities**, enabling VLMs to operate in **dynamic, simulated embodied environments**.  

By augmenting LLaVA with **temporal sequence modeling**, this project demonstrates:  

- **Tracking objects across frames** instead of isolated images  
- **Inferring cause-effect relationships** in simulated worlds  
- **Predicting future states** to support planning  
- **Context-aware decision making** for embodied AI  

ğŸ’¡ In short: Temporal LLaVA Habitat transforms static perception into **temporal intelligence**, bridging the gap between seeing and acting in real-world robotics.  

---

## âš¡ The Challenge: Beyond Static Understanding  

Most multimodal models can answer:  
- _â€œWhat is in this picture?â€_  

But they fail at:  
- _â€œWhere did the object move?â€_  
- _â€œWhat happened just before this?â€_  
- _â€œWhat will happen next?â€_  

This project addresses that temporal blindness. For example:  
- A service robot tracking a mug that was moved from kitchen to table.  
- A drone anticipating a moving obstacle rather than reacting too late.  

**Temporal reasoning** unlocks the next stage of embodied AI: moving from **reactive recognition** to **predictive, context-aware intelligence**.  

---

## ğŸ“‚ Project Structure  

```

embodied-temporal-reasoning/
â”‚â”€â”€ main.py                # Core training and evaluation scripts
â”‚â”€â”€ models/                # Temporal extensions of LLaVA
â”‚â”€â”€ habitat\_env/           # Embodied AI simulation environments
â”‚â”€â”€ configs/               # Experiment configurations
â”‚â”€â”€ benchmarks/            # Evaluation metrics and results
â”‚â”€â”€ screenshots/           # Demo screenshots
â”‚â”€â”€ requirements.txt       # Dependencies
â”‚â”€â”€ README.md              # Project documentation

````

---

## ğŸ–¼ï¸ Screenshots  

| Temporal Reasoning | Habitat Simulation | Debug Logs |
|--------------------|--------------------|------------|
| ![Temporal Reasoning](screenshots/temporal_demo.png) | ![Habitat Environment](screenshots/habitat_scene.png) | ![Debug](screenshots/debug_logs.png) |

---

## âš™ï¸ Installation  

### 1. Clone this repository  
```bash
git clone https://github.com/Rukh-sana/embodied-temporal-reasoning.git
cd embodied-temporal-reasoning
````

### 2. Install dependencies

We build on [Habitat-Sim](https://github.com/facebookresearch/habitat-sim). Please follow the [official installation guide](https://github.com/facebookresearch/habitat-sim#installation).

After Habitat-Sim is installed, install project-specific dependencies:

```bash
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸš€ Usage

Run training/evaluation inside Habitat with temporal reasoning:

```bash
python main.py --config configs/temporal_llava.yaml
```

Available configuration files are in [`configs/`](configs).

---

## ğŸ“Š Benchmarks

Temporal LLaVA was evaluated on **Habitat embodied tasks**:

* âœ… **Object tracking across frames** â†’ 23% improvement over static LLaVA
* âœ… **Temporal Question Answering** â†’ Better accuracy in multi-frame queries
* âœ… **Predictive Planning** â†’ Higher success rate in navigation tasks

Detailed benchmark tables are available in [`benchmarks/`](benchmarks).

---

## ğŸ“š Citation

If you use this repository, please cite:

```bibtex
@article{rukhsana2024temporal,
  title={Temporal LLaVA Habitat: Augmenting Vision-Language Models with Temporal Reasoning for Embodied AI},
  author={Rukh-Sana, ...},
  journal={Preprint},
  year={2024}
}
```

---

## ğŸ”® Future Work

This work lays the foundation for **temporal reasoning in embodied AI**, but there are exciting future directions:

* **Aerial Robotics & Drones** â€“ Extend temporal reasoning to aerial navigation tasks, integrating **reinforcement learning** for obstacle avoidance and trajectory prediction.
* **Transformers in Robotics** â€“ Use transformer-based architectures for **long-horizon temporal memory**, vital for drones, service robots, and autonomous systems.
* **Cross-Modal Integration** â€“ Combine **vision, language, proprioception, and environment maps** to improve generalization in unseen environments.
* **Real-World Deployment** â€“ From household assistants to drone fleets, deploy temporal reasoning beyond simulation into robotics platforms.

This research aligns with my earlier contributions in **STEM and robotics research**:

* [STEM Journal Paper](https://www.lcjstem.com/index.php/jstem/article/view/101)
* [Wiley Publication](https://onlinelibrary.wiley.com/doi/abs/10.1002/2050-7038.12504)

Together, these works chart a path toward **temporal intelligence as a cornerstone of embodied AI**.

---

## ğŸ¤ Contributing

Contributions are welcome! Please open an issue or pull request if youâ€™d like to extend this work.

---

## ğŸ“œ License

This project is licensed under the MIT License.

```
